{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keywords_selection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnLVR1oPbQLCDjazOtBy7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi-mustapha/PROJETS-JEDHA/blob/main/keywords_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Foreword** \n",
        "\n",
        "Charger les fichiers de mots-clés\n",
        "Ce notebook montre comment sélectionner efficacement des mots-clés pertinents à partir de recherches multiples sur Google Ads (> 400 fichiers pour 100K expressions dans le cas présent).\n",
        "\n",
        "A la réflexion, s'il fallait industrialiser le processus, un module devrait être développé pour gérer directement les appels api sur Google Ads."
      ],
      "metadata": {
        "id": "IrI23RV8fvVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Packages**"
      ],
      "metadata": {
        "id": "Mb6oG4rKf7G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy -q\n",
        "!python -m spacy download fr_core_news_sm -q\n",
        "!pip install unidecode\n",
        "!pip install unidecode\n",
        "!pip install wordcloud -q\n",
        "!pip install stylecloud\n",
        "!pip install matplotlib==3.4\n",
        "!pip install random2"
      ],
      "metadata": {
        "id": "0lX_9Yopu2PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "import fr_core_news_sm\n",
        "import string\n",
        "import unidecode\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from fnmatch import fnmatch\n",
        "import random\n",
        "\n",
        "# Import matplotlib and wordcloud \n",
        "import chardet\n",
        "import spacy\n",
        "import fr_core_news_sm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import warnings; warnings.filterwarnings(action='ignore')\n",
        "import time\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "large = 18; med = 14; small = 11\n",
        "params = {'axes.titlesize': large,\n",
        "          'legend.fontsize': small,\n",
        "          'figure.figsize': (16, 8),\n",
        "          'axes.labelsize': small,\n",
        "          'axes.titlesize': small,\n",
        "          'xtick.labelsize': small,\n",
        "          'ytick.labelsize': small,\n",
        "          'figure.titlesize': med}\n",
        "plt.rcParams.update(params)\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "sns.set_style(\"white\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXlN8Pn4gfGf",
        "outputId": "e598c0c1-d372-473d-d61f-7cbf54de2148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trash = pd.read_csv('/content/drive/MyDrive/111_goldfinger/data_collection/all_pages/+++conso.csv')\n",
        "trash.columns"
      ],
      "metadata": {
        "id": "agxKSiL_vI5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All keywords have been exported from Google Ads"
      ],
      "metadata": {
        "id": "Kw7bS1x9h7pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/111_goldfinger/kw/all_files')\n",
        "\n",
        "\n",
        "# List all keyword files\n",
        "pattern = \"Keyword Stats*.csv\"\n",
        "i=0\n",
        "liste = []\n",
        "root = '/content/drive/MyDrive/111_goldfinger/kw/all_files'\n",
        "for path, subdirs, files in os.walk(root):\n",
        "    for name in files:\n",
        "        if fnmatch(name, pattern):\n",
        "            liste.append(name)\n",
        "            i+=1\n",
        "print('Files count : ', len(liste))\n",
        "\n",
        "# Detecting encoding\n",
        "with open(random.choice(liste), 'rb') as file:\n",
        "    encoding = chardet.detect(file.read())\n",
        "    var = ''.join(encoding['encoding'].split(':'))\n",
        "print('Necessary encoding to use for integration : ',var)\n",
        "\n",
        "# Csv compilation\n",
        "cols = ['Keyword', 'Avg. monthly searches', 'Competition (indexed value)']\n",
        "df = pd.DataFrame(columns=cols)\n",
        "\n",
        "for k,v in enumerate(liste):\n",
        "  try:\n",
        "    temp = pd.read_csv(liste[k],usecols=cols, encoding=var, sep= '\\t',skiprows=2)\n",
        "    df = pd.concat([df, temp],axis=0)\n",
        "    if k % 100 == 0:\n",
        "      print('*****  ', k, 'done ! Remaining ', len(liste) - k, ' files to load... *****')\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "new_cols = ['keyword', 'volume', 'competition']\n",
        "df = df.rename(columns=dict(zip(df.columns, new_cols)))\n",
        "\n",
        "#Copy in any case\n",
        "# df.to_csv('all_kw.csv')\n",
        "\n",
        "print()\n",
        "print(\"Done. Shape of the df DataFrame : \", df.shape)"
      ],
      "metadata": {
        "id": "E10azG7FvTqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(3)"
      ],
      "metadata": {
        "id": "yF9bmfDbvY67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has generated a lot of \"noise\" and a quick cleaning is necessary"
      ],
      "metadata": {
        "id": "gGnKgYZtiV_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First easy selection\n",
        "df1 = df[df.keyword.str.contains('emploi|cdi|cdd|interim|intérim|caces', na=False, case=False) & (df.volume > 1)]\n",
        "print(df1.shape)"
      ],
      "metadata": {
        "id": "JJoiZRp3vdZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.sample()"
      ],
      "metadata": {
        "id": "TLnMLj0k0dsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistic cleaning\n",
        "df2 = df.copy()\n",
        "\n",
        "def clean(c):\n",
        "   \n",
        "    c = c.lower()\n",
        "    # c = re.sub(\"\\d+\", \" \", c) #chiffres    \n",
        "    c = unidecode(c)\n",
        "    c = c.replace('offre d emploi', 'offre_d_emploi')\n",
        "    c = c.replace('offre emploi', 'offre_emploi')\n",
        "    c = re.sub(\"(\\s?)([0-9]{1,1000})\", \" \", c)  #digits\n",
        "    c = re.sub(\"(\\s)([a-z]{1})\\s \", \" \", c)    #lettres seules\n",
        "    c = c.replace('  ', ' ')\n",
        "    c = c.strip()\n",
        "    if c is not None:\n",
        "      if c not in STOP_WORDS:\n",
        "        return c\n",
        "df2['cc'] = df2.apply(lambda x: clean(x['keyword']), axis=1)"
      ],
      "metadata": {
        "id": "YPfsbnly-GXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.sample(5)"
      ],
      "metadata": {
        "id": "IeS_Hrcn-MnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the keywords with volume\n",
        "empty = (df2.cc =='') | (df2.cc.isna()) \n",
        "print(df2[empty].shape)\n",
        "df2 = df2[(~empty) & (df2.volume > 10) ]\n",
        "df2 = df2.drop_duplicates(subset=['cc'])\n",
        "df2.sample(5)"
      ],
      "metadata": {
        "id": "mFlNC6fy-S__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of expression for statistic analysis\n",
        "df2.cc.values"
      ],
      "metadata": {
        "id": "n12E6ai9-cnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.dropna(inplace=True)\n",
        "new = ''.join(df2.cc.values)\n",
        "tokens =  pd.DataFrame(Counter(new.split(' ')).most_common())\n",
        "#Save our list of tokens for further analysis\n",
        "tokens[0].to_csv('/content/tokens.csv')"
      ],
      "metadata": {
        "id": "HTYUDasw-ltW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compte tenu de leur fréquence, nous examinons rapidement les mots-clés hors-sujet d'un point de vue sémantique.\n",
        "\n",
        "⚡ Nous détectons avec le fichier tokens.csv, les termes hors-sujet : marques de voitures, pneus et motos, marques de téléphones et d'ordinateurs, sports, géographie, .... Ces termes sont concaténés et assignés à la variable 'pattern'.\n",
        "\n",
        "\n",
        "\n",
        "Given their frequency, we quickly look at the off-topic keywords from a semantic point of view  \n",
        "\n",
        "⚡ We detect with the `tokens.csv` file, the off-topic terms: car brands, tires and motorcycles, phone and computer brands, sports, geography, ... \n",
        "These terms are concatenated and assigned to the 'pattern' variable"
      ],
      "metadata": {
        "id": "ezeksO5njVSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = 'peugeot |bilan|attestation|pile|abarth|amazon|renata|logitech|master|agenda|cabin|crossclimate|association|duracell|porsche |max |gti |auchan|formulaire|geforce |ktm |gt |air |gb |hp |balance |phone |carrera |super |duke |prix |turbo |romeo |adventure |gaming |rx |garmin |alfa |martens |nike |ducati |canon |honda |bmw |cartouche |sw |rs |edge |agent |levis |code |gbgtx |xl |encre |boxster |asus |new |ferrari |occasion |samsung |go |pas |msi |nvidia |targa |gts |strix |office |coupe |note |fiat |abarth |line |noir |cayman |kawasaki |xp |puretech |ram |rog |forerunner |cabriolet |zxr |gti |sporsche |gigabyte |black |smc |react |android |xs |yamaha |prixpeugeot |windows |gbmsi |tigtx |ocgtx |hdi |husqvarna |italia |occasionporsche |armor |scott |jordan |davidson |gbnvidia |ggtx |soignante |pascal |miui |pista |spyder |ninja |microsoft |dodge |gbgeforce |gbgigabyte |goiphone |aorus |pg |xiaomi |stihl |epson |adventurektm |tondeuse |prixporsche |windforce |iron |break |gps |gris |departement |cabrioletporsche |gbrx |xlcartouche |suzuki |imprimante |ssd |blanche |redmi |socket |gtporsche |roomba |amp |rugby |itx |chromebook |dr |hybride |occasionpeugeot |enduro |amazon |avis |phoenix |peugeotpeugeot |lite |levi |pixma |rally |apple |forza |intel |gasus |music |toner |white |grise |beige |prime |gogtx |volvo |btwin |skoda |achat |tarif |xasus |carte |huile |vario |gtktm |visio |brown |hobby |david |santa |excel |pompe |trail |gbxfx |barca |touch |color |crush |jesus |glide |swift |dacia |magna |pixel |zasus |pasus |belle |prixs |ibiza |punto |ready |conan |cerfa |payer |solde |golf |acer |vw |rose |jantes |akrapovic |roulement |notebook |ecran |pitaka |huawei |jailbreak |cdiscount |ali express |chaine |poco |oxygen |book |boursorama |reanault |panasonic |bw |chaussure |compresseur |echappement |xtrons |antenne |youtube |catalogue |peugeotkm |cvpeugeot |clio |viking |logiciel |zalandoa |newbalance |adidas |outlook |mixpeugeot |consommation |mutpeugeot |citroen |bbs |galaxy |configurateur |webasto |argus |carplay |aramis |boite |mlmenew |pgbk |amphanew |amd |pescarolo |bafa |focal |proto |camping |configurator |mercedes |jamoatmospeugeot |dbncthorn |philips |harley |aperto |moteur |volkswagen |pieces |cbr |velocompact |interieur |zcaecartouche |bolt |aliexpress |irobot |video |radio |wallpaper |phare |pression |vidange |airbag |nokia |flight |cesu |support |largeur |partes |grimbergen |uefa |maserati |penthouse |malavita |liasse |impot |iban |fiscal |formulaire |attestation |annexe |pdf |soft |foot |food |laptop | pro |calendrier |sfr |free |orange |magicbook |oneplus |avis  |new balance |scooter |scolaire |crédit agricole |credit agricole |dpt  |semaine |michelin |pirelli |pneu |moto |d2partement |dep  |reconditionn |estanguet| cher |imprim |fisca|gratuit|1|2|3|4|5|6|7|8|9|0|gtx |tignieu|superduke|veretz|dept| var|pyrénée|pyrene|yveline|xadv|fonky|heim|alsace|diavel|world|general|moselle|chevilly|marne|levi\\'s|daytona|constantinople|essonne|chelsea|barcelon|harley|pref|mbk|pfizer|préf|ville|region|région|yvelynes|sortir|paris|fb|gsxf|saint| en | sur |seine|yvelynes|france|danemark|www|facebook|psg|argent|insta|cent|boxster|yeezy|film|boost|soft|vespa|meteofr|corona|virus|ducati|adidas|zidane|meteofrance|covid|département|steel|delonghi|cpam'\n",
        "df2 = df2[~df2.keyword.str.contains(pattern, na=False, case=False)]\n",
        "df2"
      ],
      "metadata": {
        "id": "912EpjS8_ZWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # We concatenate the two batches of correct keywords\n",
        "data  =  pd.concat([df1, df2], axis=0).drop_duplicates(subset=['keyword'])\n",
        "data = data[(data.volume > 1) |~(data.volume.isna() )] #keywords that drive trafic !\n",
        "data.drop_duplicates(subset=['keyword'], inplace=True)\n",
        "data = data[['keyword', 'volume', 'competition']]\n",
        "data"
      ],
      "metadata": {
        "id": "LYXHdwJg_luQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.competition.fillna(1, inplace=True)  #no empty values"
      ],
      "metadata": {
        "id": "iEsmwCXG_vIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('/content/drive/MyDrive/111_goldfinger/data_collection/results/correct_keywords.csv')"
      ],
      "metadata": {
        "id": "_uYlWHIvADxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistic display of the most frequent terms**"
      ],
      "metadata": {
        "id": "ZzuD8P4FkBm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "font_path = '/content/drive/MyDrive/111_goldfinger/Comfortaa-Bold.ttf'\n",
        "mask2 = np.array(Image.open(\"/content/drive/MyDrive/111_goldfinger/cloud2.jpg\"))\n",
        "wd = wordcloud.WordCloud(\n",
        "                        # min_font_size=8,\n",
        "                        #  max_font_size=25, \n",
        "                         max_words=1000, \n",
        "                         background_color=\"#145774\", \n",
        "                         colormap='tab20c', \n",
        "                         mask = mask2,\n",
        "                         font_path=font_path,\n",
        "                         stopwords=STOP_WORDS)\n",
        "\n",
        "# Generate word cloud \n",
        "cloud = wd.generate(''.join(data.keyword.values))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Show word cloud with matplotlib \n",
        "my_dpi=150\n",
        "plt.figure(figsize = (15,15), dpi = my_dpi)\n",
        "# plt.figure()\n",
        "plt.imshow(cloud)\n",
        "\n",
        "# Remove trailing logs \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xmEbDIhXAWRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud.to_file('/content/drive/MyDrive/111_goldfinger/data_collection/results/adwords.png')"
      ],
      "metadata": {
        "id": "WY23BCbpAj-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}